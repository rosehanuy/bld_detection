{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6567c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import stackstac\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "from shapely import Point\n",
    "import math as math\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import folium\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ace28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentinel_tiles(boundary):\n",
    "    s2_grid_url = \"https://unpkg.com/sentinel-2-grid/data/grid.json\"\n",
    "    grid_gdf = gpd.read_file(s2_grid_url)\n",
    "\n",
    "    # find tiles that intersect with AOI\n",
    "    bb = boundary.boundary.to_crs(grid_gdf.crs)\n",
    "    tiles = grid_gdf[grid_gdf.intersects(bb.geometry.iloc[0])]\n",
    "    exploded = tiles.explode()[0:] # separate geometry collection into single geometries\n",
    "\n",
    "    m = folium.Map(location=(42.44,-76.21), zoom_start=5, tiles='OpenStreetMap')\n",
    "\n",
    "    folium.GeoJson(\n",
    "        exploded,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=[\"id\"])\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.GeoJson(bb).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def get_sentinel_tiles(boundary):\n",
    "    \"\"\"Get tile ids that overlap boundary\"\"\"\n",
    "    \n",
    "    s2_grid_url = \"https://unpkg.com/sentinel-2-grid/data/grid.json\"\n",
    "    grid_gdf = gpd.read_file(s2_grid_url)\n",
    "\n",
    "    # find tiles that intersect with AOI\n",
    "    bb = boundary.boundary.to_crs(grid_gdf.crs)\n",
    "    tiles = grid_gdf[grid_gdf.intersects(bb.geometry.iloc[0])]\n",
    "    \n",
    "    return [t for t in tiles['name']]\n",
    "\n",
    "\n",
    "\n",
    "def remove_outliers(a,min,max):  # slavi: 0-20, psri: -0.5 - 1.0\n",
    "    #a = a.where(np.isfinite(a),np.nan)\n",
    "        a = xr.where(np.isfinite(a),a,np.nan)\n",
    "        a = a.clip(min=min,max=max)\n",
    "        return a\n",
    "\n",
    "def get_variables(data):\n",
    "        blue = data.sel(band='B02')\n",
    "        red = data.sel(band='B04')\n",
    "        nir = data.sel(band='B8A')\n",
    "        sw1 = data.sel(band='B11')\n",
    "        sw2 = data.sel(band='B12')\n",
    "        re2 = data.sel(band='B06')\n",
    "\n",
    "\n",
    "        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1)).expand_dims({'band':['evi']}) \n",
    "        lswi = (nir - sw1)/(nir + sw1).expand_dims({'band':['lswi']}) \n",
    "        slavi = nir/(red + sw2).expand_dims({'band':['slavi']}) \n",
    "        psri = (red - blue)/re2.expand_dims({'band':['psri']}) \n",
    "\n",
    "        # bright = (0.3510*blue)+(0.3813*green)+(0.3437*red)+(0.7196*nir)+(0.2396*sw1)+(0.1949*sw2).expand_dims({'band':[f'bright_{season_list[i]}']})\n",
    "        # bright = self.remove_outliers(bright)\n",
    "\n",
    "        # wet = (0.2578*blue)+(0.2305*green)+(0.0883*red)+(0.1071*nir)+(-0.7611*sw1)+(-0.5308*sw2).expand_dims({'band':[f'wet_{season_list[i]}']})\n",
    "        # wet = self.remove_outliers(wet)\n",
    "\n",
    "        # green = (-0.3599*blue)+(-0.3533*green)+(-0.4734*red)+(0.6633*nir)+(0.0087*sw1)+(-0.2856*sw2).expand_dims({'band':[f'green_{season_list[i]}']})\n",
    "        # green = self.remove_outliers(green)\n",
    "\n",
    "        evi = remove_outliers(evi,min=-1,max=1)\n",
    "        lswi = remove_outliers(lswi,min=-1,max=1)\n",
    "        slavi = remove_outliers(slavi,min=0,max=20)\n",
    "        psri = remove_outliers(psri,min=-0.5,max=1.0)\n",
    "\n",
    "        all_vars = xr.concat([data,evi,lswi,slavi,psri],dim='band')\n",
    "        # ## mask non-forest pixels\n",
    "        # self.all_variables = m3.where(m3.sel(band=f'evi_month{num_of_peak_evi_month}')>0.5,other=np.nan)\n",
    "        #all_vars = xr.where(all_vars.isel(time=1,band=0)>=0.5,all_vars,np.nan)\n",
    "\n",
    "        return all_vars\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_gradient(im) :\n",
    "        # Calculate the x and y gradients using Sobel operator\n",
    "        grad_x = cv2.Sobel(im,cv2.CV_32F,1,0,ksize=3)\n",
    "        grad_y = cv2.Sobel(im,cv2.CV_32F,0,1,ksize=3)\n",
    "        # Combine the two gradients\n",
    "        grad = cv2.addWeighted(np.absolute(grad_x), 0.5, np.absolute(grad_y), 0.5, 0)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "\n",
    "def coregister_data(data):\n",
    "    # open raw sentinel data file\n",
    "    #data = xr.open_dataarray(self.root / 'sentinel_data' / self.site_name / f'{self.year}_{self.site_name}.nc')\n",
    "\n",
    "    data = data.astype('float32') # convert from float64 to save memory\n",
    "    # set nans to 0\n",
    "    b_sel = xr.where(~np.isnan(data), data, 0)\n",
    "\n",
    "    # create reference image: mean of all temporal steps\n",
    "    reference_image = data.mean(dim=['band','time'])\n",
    "    # replace na with 0 \n",
    "    reference_image = xr.where(~np.isnan(reference_image),reference_image,0)\n",
    "    # convert to numpy array\n",
    "    reference_image = reference_image.to_numpy()\n",
    "\n",
    "    # define dimensions for output image\n",
    "    height = b_sel.shape[2]\n",
    "    width = b_sel.shape[3]\n",
    "    time = b_sel.shape[0]\n",
    "    band = b_sel.shape[1]\n",
    "\n",
    "    ## Define motion model\n",
    "    #warp_mode = cv2.MOTION_AFFINE\n",
    "    warp_mode = cv2.MOTION_TRANSLATION\n",
    "\n",
    "    # Set the stopping criteria for the algorithm.\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 5000,  1e-10)\n",
    "\n",
    "    # Create empty array of correct size for new aligned images\n",
    "    im_aligned = np.zeros((time,band,height,width), dtype=np.float32 )\n",
    "\n",
    "    ref_image_gradient = get_gradient(reference_image)\n",
    "\n",
    "    #im = b_sel.to_numpy()\n",
    "    # loop over time and band dimensions and apply coregistration to each band\n",
    "    # calculate warp_matrix for only one band per timestamp and apply to all bands\n",
    "    for i in tqdm(range(0,time)):\n",
    "        im = b_sel.isel(time=i).to_numpy()  # shape: (band, y, x)\n",
    "        # get just red band for calculating warp matrix\n",
    "        red_band = im[2,:,:]\n",
    "        red_gradient = get_gradient(red_band)\n",
    "        \n",
    "        # set empty warp matrix\n",
    "        warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "        # calculate warp matrix based on downsampled red band and reference image\n",
    "        (_, warp_matrix) = cv2.findTransformECC(ref_image_gradient, red_gradient, warp_matrix, warp_mode, criteria) \n",
    "        \n",
    "        # apply transformation to each band \n",
    "        for j in range(0,band):                                \n",
    "            im_aligned[i,j,:,:] = cv2.warpAffine(im[j,:,:], warp_matrix, (width,height), flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "        \n",
    "        del im\n",
    "        gc.collect() # free memory\n",
    "        \n",
    "    b_align = xr.DataArray(im_aligned, \n",
    "                    coords={'time':b_sel.time,'band': b_sel.band,'y': b_sel.y,'x':b_sel.x}, \n",
    "                    dims=['time','band','y','x'])\n",
    "    # reset 0 values to na\n",
    "    b_align = xr.where(b_align == 0, np.nan, b_align)\n",
    "\n",
    "    return b_align\n",
    "\n",
    "\n",
    "\n",
    "def build_summer_timeseries(boundary, years, tiles, cf_mosaic=True):\n",
    "    \"\"\"Downloads Sentinel-2 data between June 15th - August 15th for each year, mosaics adjacent/overlapping tiles, masks cloud/shadows/water/snow/aerosol, applies scale factor (and offset if processing baseline < 4.0), co-registers images, calculates vegetation indices, and assembles into timeseries. \n",
    "\n",
    "    If cf_mosaic = False, takes the single least cloudy image per year.\n",
    "    If cf_mosaic = True, creates median mosaic of all available images per year.\"\"\"\n",
    "\n",
    "    epsg = boundary.crs.to_epsg()\n",
    "    bbox_4326 = tuple(boundary.to_crs(4326).total_bounds)\n",
    "    bbox_utm = tuple(boundary.total_bounds)\n",
    "\n",
    "    catalog = Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "\n",
    "\n",
    "    query = {\"eo:cloud_cover\":{\"lt\":20},\n",
    "            \"s2:mgrs_tile\": {'in':tiles}} \n",
    "\n",
    "    timesteps = []\n",
    "\n",
    "    for year in years:\n",
    "        items = catalog.search(\n",
    "            bbox=bbox_4326,\n",
    "            collections=[\"sentinel-2-l2a\"],\n",
    "            datetime=f\"{year}-06-15/{year}-08-15\",\n",
    "            query= query #\"s2:mgrs_tile\": {\"eq\": tile_id},\n",
    "        ).item_collection()\n",
    "        print(f'{year}: number of images found: {len(items)}')\n",
    "        if len(items) == 0:\n",
    "            print(f'SKIPPING {year}')\n",
    "            continue\n",
    "        \n",
    "        # get images with highest (i.e. most recent) processing baseline.\n",
    "        # if highest baseline is less than 4.0, set apply offset to true. \n",
    "        baselines = []\n",
    "        for i in items:\n",
    "            baselines.append(float(i.properties.get('s2:processing_baseline')))\n",
    "        highest = np.max(baselines)\n",
    "        selected_items = [i for i in items if float(i.properties.get('s2:processing_baseline')) == highest]\n",
    "        if highest < 4.0:\n",
    "            apply_offset = True\n",
    "            print(f'processing baseline: {highest} - applying offset')\n",
    "        else:\n",
    "            apply_offset = False\n",
    "            print(f'processing baseline: {highest}')\n",
    "        \n",
    "        # create xarray\n",
    "        stack = stackstac.stack(\n",
    "            selected_items,\n",
    "            epsg=epsg,\n",
    "            resolution=10,\n",
    "            bounds=bbox_utm,\n",
    "            assets=['B02','B03','B04','B05','B06','B07','B08','B8A','B11','B12','SCL'],\n",
    "            resampling=Resampling.bilinear)\n",
    "\n",
    "\n",
    "        #stack = stack.chunk({'time':timechunk,'band':bandchunk,'y':None,'x':None})\n",
    "\n",
    "        # # # crop to boundary\n",
    "        stack = stack.rio.clip(geometries=boundary.geometry)\n",
    "\n",
    "        stack  = stack.assign_coords(time=stack['time'].dt.floor('D'))\n",
    "\n",
    "        mosaic = stack.groupby('time').median(dim='time',skipna=True) # merge images from different tiles taken on same day\n",
    "\n",
    "        mosaic = mosaic.drop_attrs()\n",
    "        mosaic = mosaic.reset_coords(drop=True)\n",
    "\n",
    "        #mosaic = mosaic.chunk({'time':None,'band':None,'y':1024,'x':1024})\n",
    "\n",
    "        mosaic = mosaic.astype('float32')\n",
    "\n",
    "        scl = mosaic.sel(band='SCL')\n",
    "        mask = ~scl.isin([1, 3, 6, 8, 9, 10, 11])\n",
    "        #mask = ~scl.isin([1, 3, 6, 8, 9, 10, 11]).persist()\n",
    "\n",
    "        masked = mosaic.where(mask).drop_sel(band='SCL')\n",
    "\n",
    "        if apply_offset:\n",
    "                scaled = (masked + 1000)/ 10000\n",
    "        else:\n",
    "                scaled = masked / 10000\n",
    "        \n",
    "        scaled = scaled.clip(min=0) # set minimum reflectance to 0\n",
    "\n",
    "        #masked = masked.where(masked > 0, other=np.nan)\n",
    "        # get single least cloudy image\n",
    "        if not cf_mosaic:\n",
    "            sample_band = scaled.isel(band=0)  \n",
    "            #valid_pixel_count = da.sum(da.isfinite(sample_band.data), axis=(1, 2))\n",
    "            valid_pixel_count = sample_band.count(dim=['y','x']).values\n",
    "            v = valid_pixel_count.argmax() # index of timestep with most valid pixels\n",
    "            scaled_lc = scaled.isel(time=v)\n",
    "            timesteps.append(scaled_lc)\n",
    "        else:\n",
    "            timesteps.append(scaled)\n",
    "\n",
    "    ts = xr.concat(timesteps,dim='time')\n",
    "\n",
    "    # align\n",
    "    print('aligning bands')\n",
    "    aligned = coregister_data(ts)\n",
    "    with_indices = get_variables(aligned)\n",
    "\n",
    "    if cf_mosaic:  # create median mosaic of all available timesteps per year\n",
    "        aggregated = with_indices.groupby('time.year').median(dim='time', skipna=True) \n",
    "        return aggregated\n",
    "    else:\n",
    "        return with_indices\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c50dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: take the single least cloudy image per year\n",
    "# version 2: create median mosaic of all images available per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b458712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: number of images found: 3\n",
      "processing baseline: 2.12 - applying offset\n",
      "2017: number of images found: 1\n",
      "processing baseline: 2.12 - applying offset\n",
      "2018: number of images found: 4\n",
      "processing baseline: 2.12 - applying offset\n",
      "2019: number of images found: 4\n",
      "processing baseline: 2.12 - applying offset\n",
      "2020: number of images found: 6\n",
      "processing baseline: 2.12 - applying offset\n",
      "2021: number of images found: 3\n",
      "processing baseline: 3.0 - applying offset\n",
      "2022: number of images found: 6\n",
      "processing baseline: 5.1\n",
      "2023: number of images found: 4\n",
      "processing baseline: 5.1\n",
      "2024: number of images found: 0\n",
      "SKIPPING 2024\n",
      "2025: number of images found: 6\n",
      "processing baseline: 5.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [05:55<00:00, 16.14s/it]\n"
     ]
    }
   ],
   "source": [
    "b = gpd.read_file(root / 'data' / 'ordway' / 'ordway_boundary.gpkg')\n",
    "boundary = b.to_crs(epsg=26917)\n",
    "\n",
    "tiles = get_sentinel_tiles(boundary)\n",
    "\n",
    "tsv2 = build_summer_timeseries(boundary,years=[2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025], tiles=tiles,cf_mosaic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05423863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beech-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
